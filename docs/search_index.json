[["index.html", "Portfolio Curriculum Vitae Contact Profile Education Experience Skills Languages", " Portfolio Youri Lam 2023-01-08 Curriculum Vitae Contact Barneveld, The Netherlands yourilam2001@gmail.com +31 6 21203760 github.com/YouriLam Profile I am a young, studious, determined, curious student with a passion for biomedical research. Although my interests mainly lie in lab work, I am also very much at home in bioinformatics. By combining both interests I see myself as an asset in the development of data analysis and biomedical research. Education Hogeschool Utrecht Bachelor of Science - BS, Life Sciences, Sep. 2019 - Aug. 2024 Specialized in Data Sciences for Biology Experience Eurofins Analytico B.V. - Barneveld Pre-treatment Organic Substances Nov. 2021 - Present Coop BV - Barneveld Sales Associate, Nov. 2016 - Nov. 2021 Skills R programming language Bash command language SQL Excel CSS stylesheet language (Cell) culture Languages Dutch - native speaker English - C1 "],["01_elegans_experiment.html", "Chapter 1 Analyzing and visualizing data 1.1 Preparing the data 1.2 Normalizing the data", " Chapter 1 Analyzing and visualizing data 1.1 Preparing the data Before the data can be visualized, it has to be cleaned up. The dataframe is already in tidy format, but the data types of the columns need to be changed. The variables RawData (the outcome - number of offspring counted as an integer value, after incubation time), compName (the generic name of the compound/chemical) and the compConcentration (the concentration of the compound) are all very important in this research. Expected data types would be double/integer for RawData, character for compName and double for compConcentration. dataset_elegans &lt;- read_excel(here(&quot;raw_data/CE.LIQ.FLOW.062_Tidydata.xlsx&quot;)) typeof(dataset_elegans$RawData) ## [1] &quot;double&quot; typeof(dataset_elegans$compName) ## [1] &quot;character&quot; typeof(dataset_elegans$compConcentration) ## [1] &quot;character&quot; The actual data types of the columns are double for RawData, character for compName and character for compConcentration, as seen above. When plotted, this will result in the following issue: plot_wrong &lt;- ggplot(data = dataset_elegans, aes(x = compConcentration, y = RawData))+ geom_point(aes(color = compName, shape = expType), size = 1)+ rotate_axis_labels(&quot;x&quot;, 90)+ labs(x = &quot;Concentration&quot;, y = &quot;Number of offspring&quot;, title = &quot;Number of offspring of adult Elegans were exposed to varying concentrations of different compounds, alphabetical&quot;, shape = &quot;Type&quot;, color = &quot;Compound&quot;)+ theme(legend.key.size = unit(0.75,&quot;line&quot;), legend.text = element_text(size = 8)) plot_wrong Figure 1.1: Number of C. elegans offspring under a number of circumstances where the x-axis is ordered alphabetically As seen in the figure above, the x-axis labels are ordered alphabetically. This is because the data type of the compConcentration is character instead of double. TO make sure the graph looks better in the future, the classes need to be changed as follows: dataset_elegans$RawData &lt;- as.integer(dataset_elegans$RawData) dataset_elegans$compName &lt;- as.factor(dataset_elegans$compName) dataset_elegans$compConcentration &lt;- as.double(dataset_elegans$compConcentration) class(dataset_elegans$RawData) ## [1] &quot;integer&quot; class(dataset_elegans$compName) ## [1] &quot;factor&quot; class(dataset_elegans$compConcentration) ## [1] &quot;numeric&quot; Looking at the compUnit part of the data, I noticed that not all of the data shares the same compUnit. Some of them are compounds measured in nM and some compounds are measured in percentage. To show this, I made sure to divide the data into two graphs, one for each unit. By changing the date types to the correct ones (including the types talked about previously) and adding jitter, we can make the graph look like this: #Scatter plot filtered on nM as unit elegans_nM &lt;- dataset_elegans %&gt;% filter(compUnit == &quot;nM&quot;) scatter_nM &lt;- ggplot(data = elegans_nM, aes(x = log10(compConcentration), y = RawData))+ geom_jitter(aes(color = compName, shape = expType), width = 0.5, height = 0.2)+ labs(x = &quot;log10 concentration of compounds (nM)&quot;, y = &quot;Number of offspring&quot;)+ theme(legend.position = &quot;none&quot;) #Scatter plot filtered on pct as unit elegans_pct &lt;- dataset_elegans %&gt;% filter(compUnit == &quot;pct&quot;) scatter_pct &lt;- ggplot(data = elegans_pct, aes(x = expType, y = RawData))+ geom_jitter(aes(color = compName, shape = expType), width = 0.1, height = 0.2)+ labs(x = &quot;log10 concentration of compounds (nM)&quot;, y = &quot;Number of offspring&quot;)+ theme(legend.position = &quot;none&quot;, axis.text.x=element_text(vjust=0.5, hjust=0.5, size = 8.75), axis.text.y = element_blank(), axis.ticks.y = element_blank(), axis.title.y = element_blank())+ rotate_axis_labels(&quot;x&quot;, 90) legend &lt;- get_legend(plot_wrong) plot_grid(plot_grid(scatter_nM, scatter_pct), plot_grid(NULL, legend, ncol = 3), rel_widths = c(1, 0.35))+ plot_annotation(&quot;Number of C. elegans offspring under\\na number of circumstances&quot;) Figure 1.2: Number of C. elegans offspring under a number of circumstances The positive control for this experiment is ethanol. The negative control for this experiment is S-medium. 1.2 Normalizing the data To make it easier to read the grap, the data needs to be normalized. To do this we adjust the negative control to a value of 1 and adjust the other values in the same way: #Obtain the mean of the RawData (negativeControl) neg_control &lt;- dataset_elegans %&gt;% filter(compName == &quot;S-medium&quot;) %&gt;% summarize(mean = mean(RawData, na.rm = TRUE)) #Use the mean to calculate fractions mutated &lt;- dataset_elegans %&gt;% filter(RawData &gt; 0) %&gt;% select(RawData, compName, compConcentration, expType) %&gt;% na.omit() %&gt;% mutate(normalized = RawData/neg_control$mean) Now that the data is normalized, we can make the graph: #create normalized plot normalized_plot &lt;- mutated %&gt;% filter(compName == &quot;2,6-diisopropylnaphthalene&quot; | compName == &quot;decane&quot; | compName == &quot;naphthalene&quot;) %&gt;% ggplot(aes(x = log10(compConcentration), y = normalized))+ geom_jitter(aes(color = compName), width = 0.5, height = 0.1)+ labs(x = &quot;log10 concentration (nM)&quot;, y = &quot;Normalized number of offspring&quot;, title = &quot;Number of C. elegans offspring as a fraction\\nof the negative control group&quot;, color = &quot;Compound&quot;)+ geom_hline(yintercept = 1, color = &quot;red&quot;) normalized_plot Figure 1.3: Number of C. elegans offspring as a fraction of the negative control group. Everything below the red line means less offspring than control C. elegans and everything above it has more offspring. For further analysation of the effect i would start by testing normality by performing a Shapiro-Wilk test. If unusual, i would normalize the data like done above. If normal, i would start an ANOVA between de different conditions. I would end with the post-hoc tests, to check in what combination of groups the difference is. "],["02_reproducible_research.html", "Chapter 2 Reproducible Research", " Chapter 2 Reproducible Research For the first part of the assignment I will have to check the reproduction of an article, based on the ‘Repita’ criteria (Sumner et al. 2020). These criteria are used to check for the reproducibility of scientific research articles. In the table below the different criteria are shown with a definition and the type of response it calls for. criteria_table &lt;- data.frame(TransparencyCriteria = c(&quot;Study Purpose&quot;, &quot;Data Availability Statement&quot;, &quot;Data Location&quot;, &quot;Study Location&quot;, &quot;Author Review&quot;, &quot;Ethics Statement&quot;, &quot;Funding Statement&quot;, &quot;Code Availability&quot;), Definition = c(&quot;A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the studyobjective.&quot;, &quot;A statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section.&quot;, &quot;Where the article’s data can be accessed, either raw or processed.&quot;, &quot;Author has stated in the methods section where the study took place or the data’s country/region of origin.&quot;, &quot;The professionalism of the contact information that the author has provided in the manuscript.&quot;, &quot;A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data.&quot;, &quot;A statement within the manuscript indicating whether or not the authors received funding for their research.&quot;, &quot;Authors have shared access to the most updated code that they used in their study, including code used for analysis.&quot;), ResponseType = c(&quot;Binary&quot;, &quot;Binary&quot;, &quot;Found Value&quot;, &quot;Binary; Found Value&quot;, &quot;Found Value&quot;, &quot;Binary&quot;, &quot;Binary&quot;, &quot;Binary&quot;)) knitr::kable(criteria_table, caption = &quot;Table with the criteria for reproducibility.&quot;) (#tab:criteria table)Table with the criteria for reproducibility. TransparencyCriteria Definition ResponseType Study Purpose A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the studyobjective. Binary Data Availability Statement A statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section. Binary Data Location Where the article’s data can be accessed, either raw or processed. Found Value Study Location Author has stated in the methods section where the study took place or the data’s country/region of origin. Binary; Found Value Author Review The professionalism of the contact information that the author has provided in the manuscript. Found Value Ethics Statement A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data. Binary Funding Statement A statement within the manuscript indicating whether or not the authors received funding for their research. Binary Code Availability Authors have shared access to the most updated code that they used in their study, including code used for analysis. Binary The research article I will be scoring for reproducibility is “Effects of taurine on resting-state fMRI activity in spontaneously hypertensive rats” (V. C.-H. Chen et al. 2017). The study objective is, if it wasn’t obvious from the title, to investigate the effects of taurine on resting-state fMRI activity in ADHD. fMRI measurement is a big factor in the pathogenesis of ADHD. The more fMRI, the more likely reduction in volume or function in specific brain areas will occur, resulting in various behavioral problems.Taurine, which is known to be the richest amino acid in the central nervous system, performs various functions in the body. One of these functions as being a neurotransmitter. Multiple findings (linked in the article itself) strongly associate taurine with various ADHD-related neurotransmitters.The goal of this article is to find out if there’s a connection between the two. Using the table previously mentioned, we can go ahead and rank our own article’s reproducibility: criteria_adhd &lt;- criteria_table %&gt;% mutate(Rating = c(&quot;Present&quot;, &quot;Present&quot;, &quot;Present, the data can be found [here](https://figshare.com/articles/dataset/SHR/5091727)&quot;, &quot;The workplace of the authors has been given (Taiwan), but the location where the research has been performed remains unnamed.&quot;, &quot;Only 2 of the authors&#39; email adresses have been given, the others are not able to be contacted&quot;, &quot;Not present&quot;, &quot;Present&quot;, &quot;Not present&quot;)) knitr::kable(criteria_adhd, caption = &quot;Table showing how the article scored on reproducibility.&quot;) (#tab:ranking article 1)Table showing how the article scored on reproducibility. TransparencyCriteria Definition ResponseType Rating Study Purpose A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the studyobjective. Binary Present Data Availability Statement A statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section. Binary Present Data Location Where the article’s data can be accessed, either raw or processed. Found Value Present, the data can be found here Study Location Author has stated in the methods section where the study took place or the data’s country/region of origin. Binary; Found Value The workplace of the authors has been given (Taiwan), but the location where the research has been performed remains unnamed. Author Review The professionalism of the contact information that the author has provided in the manuscript. Found Value Only 2 of the authors’ email adresses have been given, the others are not able to be contacted Ethics Statement A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data. Binary Not present Funding Statement A statement within the manuscript indicating whether or not the authors received funding for their research. Binary Present Code Availability Authors have shared access to the most updated code that they used in their study, including code used for analysis. Binary Not present Overall, I give this article a 8.5/10 on reproducibility. It’s clear what the writers of this article want to put forward, even if no code is provided. For the second part of the assignment I will have to use the code provided with an article. I will have to try and understand the code, explain what it does and even try to make at least one figure. 2.0.1 The article This article (Buecker et al. 2020) describes changes in daily loneliness for German residents during the first four weeks of the COVID-19 pandemic. The code is easily available and very easily readable. There are a lot of codes included. Using the processed data, it is very easy to recreate on of the figures from this study. I would rate the readabilty of this code a 5/5. 2.0.2 The code # gg_color_hue ------------------------------------------------------------ ## function for colors gg_color_hue &lt;- function(n) { hues = seq(15, 375, length = n + 1) hcl(h = hues, l = 65, c = 100)[1:n] } # get_max_daily ----------------------------------------------------------- # x &lt;- ymd(&quot;2020-03-23 UTC&quot;) get_max_daily &lt;- function(x){ x &lt;- x + c(1:4, 9:12, 17:20, 25:27) x &lt;- x[x &lt; ymd(&quot;2020-04-13 UTC&quot;)] return(x) } # read data --------------------------------------------------------------- # sample 1 sample1 &lt;- read.csv(here(&quot;raw_data/sample1_use for revision 1.csv&quot;)) sample1 &lt;- sample1[sample1$include_1,] # sample 2 sample2 &lt;- read.csv(here(&quot;raw_data/sample2_use for revision 1.csv&quot;)) sample2 &lt;- sample2[sample2$include_1,] # combine samples for the additional analyses data &lt;- rbind(sample1, sample2) # packages ---------------------------------------------------------------- # install.packages(&quot;lubridate&quot;) library(lubridate) # ------------------------------------------------------------------------- ## define colors c &lt;- gg_color_hue(2) col.s1 &lt;- c[1] col.s2 &lt;- c[2] # completed baseline surveys per day that are included in the study data_l2_s1 &lt;- unique(sample1[c(&quot;ID&quot;, &quot;b_baseline_ended&quot;)]) data_l2_s2 &lt;- unique(sample2[c(&quot;ID&quot;, &quot;b_baseline_ended&quot;)]) # no missings on this variable any(is.na(data_l2_s1[2])) ## [1] FALSE any(is.na(data_l2_s2[2])) ## [1] FALSE date_s1 &lt;- ymd_hms(data_l2_s1$b_baseline_ended) hour(date_s1) &lt;- 0 minute(date_s1) &lt;- 0 second(date_s1) &lt;- 0 date_s2 &lt;- ymd_hms(data_l2_s2$b_baseline_ended) hour(date_s2) &lt;- 0 minute(date_s2) &lt;- 0 second(date_s2) &lt;- 0 # number of baseline surveys completed t_date_baseline_1 &lt;- table(ymd(date_s1)) t_date_baseline_2 &lt;- table(ymd(date_s2)) t_date_baseline_1 &lt;- c(t_date_baseline_1, &quot;2020-04-12&quot; = 0) t_date_baseline_12 &lt;- rbind(t_date_baseline_1, t_date_baseline_2) # barplot sample 1 and 2 baseline participation --------------------------- layout(matrix(1:2, 1, 2, byrow = TRUE)) b &lt;- barplot(t_date_baseline_12, beside = TRUE, ylim = c(0, 800), names.arg = rep(&quot;&quot;, length(t_date_baseline_12)), col = c(col.s1, col.s2), axes = FALSE) box() axis(2, las = 2, cex.axis = 0.8) s &lt;- seq(1, 28, 7) labels &lt;- colnames(t_date_baseline_12)[s] axis(1, at = ((b[1,] + b[2,])/2)[s], labels = labels, cex.axis = 0.8) text(&quot;A&quot;, x = ((b[1,] + b[2,])/2)[2], y = 800*.9, cex = 4, col = &quot;grey&quot;) legend(x = ((b[1,] + b[2,])/2)[15], y = 800, legend = c(&quot;Sample 1&quot;, &quot;Sample 2&quot;), fill = c(col.s1, col.s2), bty = &quot;n&quot;) # dates at which a daily survey could have been completed: max_daily_dates_s1 &lt;- lapply(ymd(date_s1), get_max_daily) max_daily_dates_s2 &lt;- lapply(ymd(date_s2), get_max_daily) max_dates_s1 &lt;- do.call(&quot;c&quot;, max_daily_dates_s1) max_dates_s2 &lt;- do.call(&quot;c&quot;, max_daily_dates_s2) obtained_dates_s1 &lt;- sample1$daily_date obtained_dates_s2 &lt;- sample2$daily_date t_max_dates_s1 &lt;- table(max_dates_s1) t_max_dates_s2 &lt;- table(max_dates_s2) t_obtained_dates_s1 &lt;- table(obtained_dates_s1) t_obtained_dates_s2 &lt;- table(obtained_dates_s2) t_obtained_dates_s12 &lt;- rbind(t_obtained_dates_s1, t_obtained_dates_s2) b &lt;- barplot(t_obtained_dates_s12, beside = TRUE, ylim = c(0, 2000), names.arg = rep(&quot;&quot;, length(t_obtained_dates_s12)), col = c(col.s1, col.s2), axes = FALSE) box() axis(2, las = 2, cex.axis = 0.8) labels &lt;- colnames(t_obtained_dates_s12)[s] axis(1, at = ((b[1,] + b[2,])/2)[s], labels = labels, cex.axis = 0.8) lines(y = t_max_dates_s1, x = b[1,], type = &quot;p&quot;, pch = &quot;-&quot;, col = col.s1, cex = 1.5) lines(y = t_max_dates_s2, x = b[2,], type = &quot;p&quot;, pch = &quot;-&quot;, col = col.s2, cex = 1.5) text(&quot;B&quot;, x = ((b[1,] + b[2,])/2)[2], y = 2000*.9, cex = 4, col = &quot;grey&quot;) legend(x = ((b[1,] + b[2,])/2)[15], y = 2000, legend = c(&quot;Sample 1&quot;, &quot;Sample 2&quot;), fill = c(col.s1, col.s2), bty = &quot;n&quot;) I didn’t have any trouble reading the code, or understanding what needed to happen. The only error I got was in the read.csv function (lines 70 &amp; 74). To fix this. I redirected the function with the ‘here’ command. That way, it was able to find the files in the raw_data folder. References "],["03_guerilla_framework.html", "Chapter 3 Data Management", " Chapter 3 Data Management To keep my data manageable I use the Guerilla Analytics framework. Using this method, every project/part of my lessons are in different folders. There is also a README included to give extra information about the way it’s set up. For this assignment, we had to make our DAURII workspace tidy using this framework. You can see that is done here below: dir_tree(&quot;C:/Users/Youri/OneDrive/Bureaublad/daur2&quot;) knitr::include_graphics(here(&quot;images/directory_tree.png&quot;)) Figure 3.1: Screenshot of my directory tree "],["04_genexpressie.html", "Chapter 4 Gen Expressie 4.1 Introduction 4.2 Endgoal", " Chapter 4 Gen Expressie 4.1 Introduction As of recently, the Institute of Life Sciences (ILC) can perform long-read-sequencing using a MinION sequencer. The MinION sequencer is a small and portable device capable of performing DNA sequencing. DNA sequencing, the process of reading part or all of the DNA of an organism, can be done in both a long-read method or short-read method. Long read sequencing has certain advantages and disadvantages over short-read-sequencing. The pro of using long-read-sequencing is, obvious by the name, that it’s able to read longer seqeunces. A downside to long-read sequencing is that the accuracy per read can be much lower than that of short-read sequencing. The ILC has used the MinION for various reasons, such as getting an idea of gene expression by performing RNA sequencing on human cells. Recently, RNA sequencing has been used to generate RNA sequencing data for MCF7 cells. MCF7 cells are commonly used in studies that focus on the development of cancer treatment. Due to the number of variants available, it has applications in development of chemotherapeutic drugs and understanding drug resistance. Since the usage of the MinION sequencer is relatively new the ILC wants to know whether the data they are generating is of good quality. In order to accomplish this the ILC wants to compare their data to published data about previous research surrounding MCF7 cells and their expression. A study has been found which has performed RNA sequencing using a MinION sequencer on MCF7 cells and has made their data public Y. Chen et al. (2021). 4.2 Endgoal In order to make the comparison reproducible for future data, an R package needs to be written that follows the exact method of data manipulation used in the paper. To start, we received the fastq-files as a result of the sequencing. These fastq-files will be used as input for a Rmarkdown file, containing bash scripts that will not only check the quality of these files, but also convert them into .BAM files. The quality check is an important start of the process, we obvously want to check if the sequencing gave us good enough results before we even think about visualising the expression. If the quality is deemed good enough, the .BAM files will be used as input for the functions that will be in our R package. First of all, they will be aligned with a reference genome using Minimap2. After they’ve been aligned, Bambu will be used for transcript quantification. Finally, differential gene expression will be analysed by DESeq2. After successfully processing the generated data through the pipeline the quality of the results from the ILC can be compared to that of the paper, to get an idea of the quality. Between all these steps (and at the end) the data will be visualised in graphs such as scatterplots, heatmaps etc.This will all be sent to our tutor, Chris van Oevelen. References "],["05_parameter.html", "Chapter 5 Parameterized report on COVID-19 5.1 Parameterized reports 5.2 The parameterized COVID-19 report 5.3 Altering parameters", " Chapter 5 Parameterized report on COVID-19 5.1 Parameterized reports When using a dataset with a huge ariety of countries for instance, parameters are used to make navigating this dataset easier. Using parameters, it is easier to choose exactly what you want to visualise.In this report I will be visualizing COVID-19 data obtained from ECDC about daily numbers of newly reported COVID-19 cases and deaths in EU countries.). 5.2 The parameterized COVID-19 report 5.2.1 The parameters The parameters I will be using are country, year and month. Using these parameters, I will generate a interactive plot for the newly reported number of cases and one for the newly reported number of deaths.I will be focusing on the Netherlands, as well as 2 neighbouring countries in the year 2021, october to december. The reason for this is that I was infected with COVID during this period, and was curious to see how many others had the unfortunate same fate. 5.2.2 Rendering the report The parameters are chosen when rendering the report, and are set as seen below. #generate the report with chosen parameters rmarkdown::render(&quot;06_parameter.Rmd&quot;, params = list(country = c(&quot;Netherlands&quot;, &quot;Belgium&quot;, &quot;Germany&quot;), year = 2021, month = 10:12)) For the parameterization to work, the YAML header of the markdown file needs to contain some defaults for the parameters that are used when the parameter is not specified when rendering. --- params: country: &quot;Netherlands&quot; year: 2022 month: 10 --- library(tidyverse) library(plotly) library(scales) library(here) data &lt;- read.csv(here(&quot;raw_data/data.csv&quot;)) #load the data After the data is loaded, it can be filtered to use the given parameters. data_filtered &lt;- data %&gt;% filter(countriesAndTerritories %in% params$country, year %in% params$year, month %in% params$month) #filter for the given parameters Finally, the data can be plotted: plot_cases &lt;- ggplot(data_filtered, aes(x = date, y = cases, group = countriesAndTerritories, color = countriesAndTerritories))+ geom_line()+ geom_point(size = 1)+ labs(title = &quot;Number of newly reported COVID-19 cases over time by country&quot;, y = &quot;Number of COVID-19 cases&quot;, x = &quot;Date&quot;, color = &quot;Country&quot;) plot_cases Figure 5.1: Number of reported COVID-19 cases over time by country, with on the y-axis the number of COVID-19 cases and on the x-axis the month. plot_deaths &lt;- ggplot(data_filtered, aes(x = date, y = deaths, group = countriesAndTerritories, color = countriesAndTerritories))+geom_line()+ geom_point(size = 1)+ labs(title = &quot;Number of newly reported COVID-19 deaths over time by country&quot;, y = &quot;Number of COVID-19 deaths&quot;, x = &quot;Date&quot;, color = &quot;Country&quot;) plot_deaths Figure 5.2: Number of reported COVID-19 deaths over time by country, with on the y-axis the number of COVID-19 deaths and on the x-axis the date. 5.3 Altering parameters To conclude, parameters make life much easier when visualising big amounts of data like this. To demonstrate I now fully understand how parameters work, I will be altering them below to show 3 other countries in a diffrent time period: #generate the plots with different parameters rmarkdown::render(&quot;06_parameterizedcovid.Rmd&quot;, params = list(country = c(&quot;Malta&quot;, &quot;Cyprus&quot;, &quot;Liechtenstein&quot;), year = 2022, month = 1:12)) data_filtered2 &lt;- data %&gt;% filter(countriesAndTerritories %in% params$country, year %in% params$year, month %in% params$month) #filter for the given parameters Finally, the data can be plotted: plot_cases2 &lt;- ggplot(data_filtered2, aes(x = date, y = cases, group = countriesAndTerritories, color = countriesAndTerritories))+ geom_line()+ geom_point(size = 1)+ labs(title = &quot;Number of newly reported COVID-19 cases over time by country&quot;, y = &quot;Number of COVID-19 cases&quot;, x = &quot;Date&quot;, color = &quot;Country&quot;) plot_cases2 Figure 5.3: Number of reported COVID-19 cases over time by country, with on the y-axis the number of COVID-19 cases and on the x-axis the month. plot_deaths2 &lt;- ggplot(data_filtered2, aes(x = date, y = deaths, group = countriesAndTerritories, color = countriesAndTerritories))+geom_line()+ geom_point(size = 1)+ labs(title = &quot;Number of newly reported COVID-19 deaths over time by country&quot;, y = &quot;Number of COVID-19 deaths&quot;, x = &quot;Date&quot;, color = &quot;Country&quot;) plot_deaths2 Figure 5.4: Number of reported COVID-19 deaths over time by country, with on the y-axis the number of COVID-19 deaths and on the x-axis the date. "],["06_Relational_Databases.html", "Chapter 6 Relational data and databases Introduction DBeaver connection Inspection Visualisations", " Chapter 6 Relational data and databases Introduction This assignment was focused on learning the basics of the SQL language and how to work this around relational data and databases. DBeaver connection I started by creating three data frames out of files and making them tidy. ## Import, first 11 lines are metadata flu_data &lt;- read_csv(&quot;raw_data/flu_data.csv&quot;, skip = 11) dengue_data &lt;- read_csv(&quot;raw_data/dengue_data.csv&quot;, skip = 11) gapminder_data &lt;- read.csv(&quot;raw_data/gapminder.csv&quot;) ## Make tidy. Gapminder doesn&#39;t need to be made tidy, just renaming some rows for future use. flu_tidy &lt;- pivot_longer(data = flu_data, cols = c(2:30), names_to = &quot;Country&quot;, values_to = &quot;Value&quot;) flu_tidy &lt;- flu_tidy %&gt;% mutate(Date = str_sub(Date, 1, 4)) flu_tidy &lt;- flu_tidy %&gt;% rename(&quot;Year&quot; = &quot;Date&quot;) dengue_tidy &lt;- pivot_longer(data = dengue_data, cols = c(2:11), names_to = &quot;Country&quot;, values_to = &quot;Value&quot;) dengue_tidy &lt;- dengue_tidy %&gt;% mutate(Date = str_sub(Date, 1, 4)) dengue_tidy &lt;- dengue_tidy %&gt;% rename(&quot;Year&quot; = &quot;Date&quot;) gapminder_data &lt;- as_tibble(gapminder_data) gapminder_data$year &lt;- as.character(gapminder_data$year) gapminder_data &lt;- gapminder_data %&gt;% rename(&quot;Year&quot; = &quot;year&quot;) gapminder_data &lt;- gapminder_data %&gt;% rename(&quot;Country&quot; = &quot;country&quot;) An important part of relational data is enabling comparison across different data frame. The data frames are now similar and can be stored into csv and rds files. # Export data frames as csv and rds write.csv(flu_tidy, &quot;flu_tidy.csv&quot;) write.csv(dengue_tidy, &quot;dengue_tidy.csv&quot;) write.csv(gapminder_data, &quot;gapminder_tidy.csv&quot;) saveRDS(flu_tidy, &quot;flu_tidy.rds&quot;) saveRDS(dengue_tidy, &quot;dengue_tidy.rds&quot;) saveRDS(gapminder_data, &quot;gapminder_tidy.rds&quot;) DBeaver and R were connected (my real password is hidden) and the tables were inserted into the ‘workflowsdb’ database in DBeaver for inspection. con &lt;- dbConnect(RPostgres::Postgres(), dbname = &quot;workflowsdb&quot;, host=&quot;localhost&quot;, port=&quot;5432&quot;, user=&quot;postgres&quot;, password=&quot;Asdfghjkl1910!&quot;) # Hide my real password dbWriteTable(con, &quot;gapminder&quot;, gapminder_data) dbWriteTable(con, &quot;flu&quot;, flu_tidy) dbWriteTable(con, &quot;dengue&quot;, dengue_tidy) Inspection Now that the tables are imported, it is required to check for certain things inside of them. The first thing I checked were the ‘NULL’ values. Checking for NULL values SELECT * FROM gapminder WHERE &quot;Year&quot; IS NULL; --make sure no important values are missing Table 6.1: 0 records Country Year infant_mortality life_expectancy fertility population gdp continent region SELECT * FROM gapminder WHERE &quot;Country&quot; IS NULL; Table 6.2: 0 records Country Year infant_mortality life_expectancy fertility population gdp continent region SELECT * FROM dengue WHERE &quot;Year&quot; IS NULL; Table 6.3: 0 records Year Country Value SELECT * FROM dengue WHERE &quot;Country&quot; IS NULL; Table 6.4: 0 records Year Country Value SELECT * FROM flu WHERE &quot;Year&quot; IS NULL; Table 6.5: 0 records Year Country Value SELECT * FROM flu WHERE &quot;Country&quot; IS NULL; Table 6.6: 0 records Year Country Value Turns out there were no ‘NULL’ values in all these tables, which is good. Next to checking for ‘NULL’ values, I also wondered what the total number of cases were. To check this, I used the following scripts. The total number of cases SELECT SUM(&quot;Value&quot;) --calculate the total number of cases FROM dengue; Table 6.7: 1 records sum 870.376 SELECT SUM(&quot;Value&quot;) --calculate the total number of cases FROM flu; Table 6.8: 1 records sum 8179518 For a good analysis of the different data they need to be joined together in one table with the important columns. I created a tabel combining all three tables. In my case, I decided to focus on the period between 2002 and 2015. See below for the SQL and R code. gapminder_filtered &lt;- gapminder_data %&gt;% filter(between(Year, 2002, 2015)) dbWriteTable(con, &quot;gapminder_filtered&quot;, gapminder_filtered) flu_dengue_combined &lt;- left_join(flu_tidy, dengue_tidy, by = c(&quot;Country&quot;, &quot;Year&quot;)) flu_dengue_gapminder_combined &lt;- left_join(flu_dengue_combined, gapminder_filtered, by = c(&quot;Country&quot;, &quot;Year&quot;)) flu_dengue_gapminder_combined &lt;- flu_dengue_gapminder_combined %&gt;% rename(&quot;Flu&quot; = &quot;Value.x&quot;) flu_dengue_gapminder_combined &lt;- flu_dengue_gapminder_combined %&gt;% rename(&quot;Dengue&quot; = &quot;Value.y&quot;) Visualisations Now that the dataset had been combined, it is possible to make visualisations of this. In my self-made Rpackage I made a function that will help a bit with visualising. With this dataset, the following graphs can be made: Life expectancy write.csv(flu_dengue_gapminder_combined, &quot;raw_data/package.csv&quot;, row.names=FALSE) life_expectancy_graph(&quot;Europe&quot;,&quot;2002&quot;) Figure 1: Bargraph of the life expectancy for every country in Europe in 2002. life_expectancy_graph(&quot;Europe&quot;,&quot;2012&quot;) Figure 2: Bargraph of the life expectancy for every country in Europe in 2015. ###One other visualisation dengue_brazil &lt;- flu_dengue_gapminder_combined %&gt;% filter(Country == &quot;Brazil&quot;) %&gt;% select(Dengue, Year) ggplot(data = dengue_brazil) + geom_col(aes(x = Year, y = Dengue, fill = Year), show.legend = FALSE) + labs(title = &quot;Development of dengue in Brazil each year&quot;) Figure 3: Bargraph of the amount of dengue cases in Brazil every year "],["07_Rpackage.html", "Chapter 7 Self-made R package Introduction", " Chapter 7 Self-made R package Introduction In this assignment I created a R package to support my portfolio. I named it ‘portfouri’, which is a combination of my own name and the word ‘portfolio’. The name was available, so I set up the project and the github repository to get started. Originally, I wanted to make a package that would reduce duplicate code in my portfolio. But I couldn’t really find any duplicate code. So, to make my life easier, I decided to write functions that would help me with this assignment. There are 4 functions available in this package: A package to support my portfolio would reduce duplicated code. But scanning through my portfolio, I noticed that there wasn’t a lot of code or duplicated code. Luckily, i found some in the “Relational data and databases” assignment that would work for this. The package that i made contains two functions: cont_finder() -&gt; Checks what continent the dataset sees your country in. country_finder() -&gt; Finds all the countries in a given continent. life_expentancy_graph() -&gt; Takes the difference in life expectancy, taken from countries all residing in the same continent, in the same year, and visualizes this. population_finder() -&gt; Finds the total population for the country, during the year of your choice. For the manual and contents of the package, please visit the portfouri github repository. "],["08_free_assignment.html", "Chapter 8 Free assignment 8.1 The plan 8.2 Issues 8.3 Solution", " Chapter 8 Free assignment For this assignment, I had the freedom to let my fantasy run wild and try and do an assignment with the eye on my future.This assignment was going to take around 40 hours. 8.1 The plan If I’m being honest, I’m not sure where I see myself in 2 years. I know my interests lie in the biomedical research branche, but I’m not sure whether that’s as an analist or a lab assistent. If you had asked me this a couple of months ago, I would have always chosen to be a lab assistent. But after doing this Data Science study, it has left me conflicted with myself. I think I still want to be a lab assistent at the end of the day, but I also would like to dive more into the data science. For now, I would like to learn more about working with relational data &amp; databases, especially using SQL. I work at Eurofins Analytico, which is a huge company. Associated companies all send their organic products to us, for us to analyse. These associated companies are all able to be found in a huge database. The organic substances, such as soil samples, are delivered and prepared for us in another floor. To see what’s coming in, we have to go through three different databases to find out what needs to be done, who the sample is coming from and when it’s due. With colleagues who are less able to use a computer, reading 3 databases is sometimes too much. My idea was therefore to write a code that would ensure that these 3 databases would be merged into 1 database. 8.2 Issues Unfortunately, my boss wasn’t too happy with my idea. Because the databases contains a lot of private information, I wasn’t allowed to tinker with them. Even if I would blur the names of the companies, it would still give a lot of information available about the way our company is run. This means that my entire plan could be thrown away. I made the assignment, I showed my boss the week before the christmas period, and he told me I couldn’t publish it. Nevertheless, I’m still gonna try and show the plan I had for this assignment. 8.3 Solution The following picture portaits my idea to make one big database. Not everything is exact, but I tried making it as good as possible. The first database we’ll call ‘clients’. In this database, all of the information on our clients is available. This was the main reason why I can’t show the exact results. This database is used to look up specific clients and see where they come from. Some substances have to be differently handled than others, for instance Belgian monsters. These have a different approach to the method than Dutch substances. From this database, I want to make sure I know which client has requested the analysis for these substances. So we’ll have to grab the column ‘full_name’, more on that later. The second database is the ‘Organic substances’ database. In this, the substances that are being brought are shown with the specific type of analysis they would like it to be tested on. The problem is, that the substances only get shown with a specific sequence of numbers, like 1000345. This means that it’s not possible to see which client sent the monster without a physical copy of their name with the monster. Another thing shown in this database is the rush code. The rush code is a color, which indicates how big of a priority it has. Yellow and green mean high priority, while red and blue mean less priority. The higher the priority, the quicker it has to be analysed. This is also seen in the column called date_Wanted and before_time. These show the dates the monsters have to be analyzed on, and before what time that has to happen. The third and final database consists of the ‘preparation’ part of our company. These people let us know through a database whether or not the substances have arrived and if they’re ready to be analysed. In other words, this database let’s us know when a substance is ready to be analyzed, but not which one or how much priority it has. The simple solution to all of this is to combine the databases into one big database. Again, I can’t show the results of the combined database, but I can show the code used to make this happen. CREATE TABLE clients_organic_substances AS SELECT clients.full_name, organic_substances.type_of_substance, organic_substances.rush_code, organic_substances.date_wanted, organic_substances.before_time; CREATE TABLE clients_organic_substances_preperation AS SELECT clients_organic_substances.full_name, clients_organic_substances.type_of_substance, clients_organic_substances.rush_code, clients_organic_substances.date_wanted, clients_organic_substances.before_time, preparation.rush_code, preparation.time_received FROM preparation LEFT JOIN clients_organic_substances ON preparation.rush_code = clients_organic_substances.rush_code; "],["09_reference.html", "Chapter 9 References", " Chapter 9 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
